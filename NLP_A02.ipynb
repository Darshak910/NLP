{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_A02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darshak910/NLP/blob/master/NLP_A02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7XwofyH6clm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtqWljrxpao5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installing pytorch-ignite\n",
        "!pip install pytorch-ignite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx8ynXrV6fX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing all the required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas.plotting import table\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
        "\n",
        "import torch\n",
        "from torch.nn import Conv1d, MaxPool1d, Flatten, Linear\n",
        "from torch.nn import L1Loss, CrossEntropyLoss, Dropout\n",
        "from torch.nn.functional import relu, softmax, sigmoid\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import SGD, Adam, SparseAdam, Adamax\n",
        "from ignite.metrics import Accuracy, Recall, Precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osVA4TfOzSKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the dataset and saving to pandas dataframe\n",
        "ds = pd.read_csv('https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv',\n",
        "                 delimiter='\\t')\n",
        "ds.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-eGL2K6_77U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving only the two required columns\n",
        "ds = ds[['Phrase', 'Sentiment']]\n",
        "ds.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bv_wPw5ecGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds['Phrase'][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxRoK0JW7nqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading the stopwords and wordnet corpus\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbJ1S5URcrlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stopwords_en = stopwords.words(\"english\")\n",
        "punctuations = \"?:!.,;'\\\"-()/\\\\{}[]|\"\n",
        "\n",
        "# Parameters to adjust to see the impact on outcome\n",
        "remove_stopwords = True\n",
        "use_stemming = True\n",
        "use_lemmatization = False\n",
        "remove_punctuations = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsHU4qyCmDEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgoP_GhjVXlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the dataset with 70% for Training set and 30% for Testing set\n",
        "# Shuffling the dataset with random state set to 2003\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    ds['Phrase'], ds['Sentiment'], test_size=0.3,\n",
        "    shuffle='True', random_state=2003)\n",
        "\n",
        "train_ds = pd.DataFrame(list(zip(x_train, y_train)), columns=['Phrase', 'Sentiment'])\n",
        "test_ds = pd.DataFrame(list(zip(x_test, y_test)), columns=['Phrase', 'Sentiment'])\n",
        "\n",
        "print(test_ds['Phrase'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrhBXGCnc8jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-processing for the Training set\n",
        "X, Y = [], []\n",
        "\n",
        "# For every single label save the Sentiment label\n",
        "for y in range(len(train_ds)):\n",
        "  label = train_ds['Sentiment'][y]\n",
        "  temp_phrase = []\n",
        "\n",
        "  # Split the phrase to words and setting the text to lowercase\n",
        "  space = str(train_ds['Phrase'][y]).lower().split(' ')\n",
        "  # print(space)\n",
        "\n",
        "  # For each word in the Phrase\n",
        "  for word in space:\n",
        "    new_word = word\n",
        "\n",
        "    # Giving conditional procedures according to stopwords and punctuations\n",
        "    if(word in stopwords_en):\n",
        "      continue\n",
        "    if(word in punctuations):\n",
        "      continue\n",
        "    \n",
        "    # Condition specifying if stemming is used or not\n",
        "    if(use_stemming):\n",
        "      # Using either Porter or Lancaster stemming\n",
        "      new_word = porter_stemmer.stem(new_word)\n",
        "      # new_word = lancaster_stemmer.stem(new_word)\n",
        "    \n",
        "    # Condition specifying if lemmatization is used or not\n",
        "    if(use_lemmatization):\n",
        "      new_word = wordnet_lemmatizer.lemmatize(new_word)\n",
        "\n",
        "    # Adding normalized word to a temporary variable\n",
        "    temp_phrase.append(new_word)\n",
        "\n",
        "  # if(not temp_phrase == []):\n",
        "  X.append(temp_phrase)\n",
        "  Y.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXx_7J_diOvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqr1w7axt-pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Joining the phrase\n",
        "for i in range(len(X)):\n",
        "  X[i] = ' '.join(X[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGruaAju8-EC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNgCYcudz0Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rLD0Xa7311U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merging two lists into a tuple using zip() method and converting the lists of\n",
        "# tuples to Pandas DataFrame and setting the column name\n",
        "ds_train = pd.DataFrame(list(zip(X, Y)), columns=['Phrase', 'Sentiment'])\n",
        "\n",
        "# Replacing the blank cells with NaN in the DataFrame\n",
        "ds_train['Phrase'].replace('', np.nan, inplace=True)\n",
        "\n",
        "# Dropping all the NaN values from the DataFrame\n",
        "ds_train.dropna(subset=['Phrase'], inplace=True)\n",
        "\n",
        "len(ds_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSGX3X3-SZ_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0iXJwq1OpQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Phrases associated to sentiment labels count\n",
        "print(ds_train.Sentiment.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFtevF-RG-Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Manually counting the number of '0' labelled sentiments\n",
        "count = 0\n",
        "for y in ds_train['Sentiment']:\n",
        "  if(y == 0):\n",
        "    count+=1\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnRnfUzDM_v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving similar labelled phrases together for resampling\n",
        "y0 = ds_train[ds_train['Sentiment'] == 0]\n",
        "y1 = ds_train[ds_train['Sentiment'] == 1]\n",
        "y2 = ds_train[ds_train['Sentiment'] == 2]\n",
        "y3 = ds_train[ds_train['Sentiment'] == 3]\n",
        "y4 = ds_train[ds_train['Sentiment'] == 4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWl4dOesOSNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Randomly down sampling each label to match the count of the lowest label count\n",
        "y00 = resample(y0, replace = True, n_samples = count, random_state = 2003)\n",
        "y01 = resample(y1, replace = True, n_samples = count, random_state = 2003)\n",
        "y02 = resample(y2, replace = True, n_samples = count, random_state = 2003)\n",
        "y03 = resample(y3, replace = True, n_samples = count, random_state = 2003)\n",
        "y04 = resample(y4, replace = True, n_samples = count, random_state = 2003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-GmvRiRTXNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatinating the new down sampled dataset\n",
        "ds_train = pd.concat([y00, y01, y02, y03, y04])\n",
        "\n",
        "# ds_train.Sentiment.value_counts()\n",
        "ds_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkUrItsE1xX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforming each text into vector of word counts using CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=20000, stop_words=\"english\",\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "# Transforming each text into vector of word counts using TfidfVectorizer\n",
        "# vectorizer = TfidfVectorizer(max_features=20000, stop_words=\"english\",\n",
        "#                              ngram_range=(2, 2))\n",
        "\n",
        "x_train = vectorizer.fit_transform(ds_train[\"Phrase\"])\n",
        "y_train = ds_train[\"Sentiment\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAJgZRtGcotE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train.shape\n",
        "# x_test.shape\n",
        "# X.shape[1]\n",
        "# y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFvd3qzwI5al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting into numpy arrays\n",
        "x_train_np = x_train.toarray()\n",
        "y_train_np = np.array(y_train)\n",
        "\n",
        "print(x_train_np.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK4rEbGqjDm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a class to define our model\n",
        "class CNNMultipleClassifier(torch.nn.Module):\n",
        "  # The initialization method\n",
        "  def __init__(self, batchsize, inputs, outputs):\n",
        "    super(CNNMultipleClassifier, self).__init__()\n",
        "    self.batchsize = batchsize\n",
        "    self.inputs = inputs\n",
        "    self.outputs = outputs\n",
        "\n",
        "    # Defining the input layer (input channels, output channels, kernels)\n",
        "    self.input_layer = Conv1d(inputs, batchsize, 1)\n",
        "\n",
        "    # Maxpooling layer with kernel size\n",
        "    self.maxpooling_layer = MaxPool1d(1)\n",
        "\n",
        "    # Adding Conv1d layers\n",
        "    self.conv_layer1 = Conv1d(batchsize, 64, 1)\n",
        "    self.conv_layer2 = Conv1d(64, 128, 1)\n",
        "\n",
        "    # A Flatten layer\n",
        "    self.flatten_layer = Flatten()\n",
        "\n",
        "    # A Linear layer with (inputs, outputs)\n",
        "    self.linear_layer1 = Linear(128, 64)\n",
        "    # Adding a dropout layer for reducing overfitting\n",
        "    self.dropout_layer = Dropout(p=0.2)\n",
        "\n",
        "    # Lastly, an Output layer\n",
        "    self.output_layer = Linear(64, outputs)\n",
        "\n",
        "  # A method to feed the model\n",
        "  def feed(self, input):\n",
        "    # Reshape so that it can be fed to input layer\n",
        "    # Although 1D Conv, it expects a 3D array in 1D fashion\n",
        "    input = input.reshape((self.batchsize, self.inputs, 1))\n",
        "\n",
        "    # Run through the Relu\n",
        "    output = relu(self.input_layer(input))\n",
        "\n",
        "    # Get the output of MaxPooling\n",
        "    output = self.maxpooling_layer(output)\n",
        "\n",
        "    # Output of Conv1d layers and passing through Relu\n",
        "    output = relu(self.conv_layer1(output))\n",
        "    output = relu(self.conv_layer2(output))\n",
        "\n",
        "    # Output of flatten layer\n",
        "    output = self.flatten_layer(output)\n",
        "\n",
        "    # Output of Linear layer and pass through Relu\n",
        "    output = relu(self.linear_layer1(output))\n",
        "    # Output of Dropout layer\n",
        "    output = self.dropout_layer(output)\n",
        "\n",
        "    # Lastly, output of Output layer\n",
        "    output = self.output_layer(output)\n",
        "\n",
        "    # Get value between 0 and 1 using Sigmoid\n",
        "    # output = sigmoid(output)\n",
        "\n",
        "    # To get accuracy we get int values of 0 or 1\n",
        "    output_ = torch.round(output)\n",
        "\n",
        "    # Using Softmax\n",
        "    output = softmax(output)\n",
        "\n",
        "    return output, output_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk77xGOspvP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining our model\n",
        "batchsize = 128\n",
        "\n",
        "# (batchsize, x_columns, y_columns)\n",
        "model = CNNMultipleClassifier(batchsize, x_train.shape[1], 5)\n",
        "\n",
        "# Set the model to use GPU\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7P2NSZrvCRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to return L1Loss, Accuracy, Recall and Precision\n",
        "def model_loss(model, dataset, train = False, optimizer = None):\n",
        "  \n",
        "  # Using crossentropy loss for calculating the loss\n",
        "  cross_loss = CrossEntropyLoss()\n",
        "\n",
        "  avg_loss = 0\n",
        "  avg_accuracy = 0\n",
        "  avg_recall = 0\n",
        "  avg_precision = 0\n",
        "  count = 0\n",
        "\n",
        "  for input, output in iter(dataset):\n",
        "    # Get predictions for training dataset\n",
        "    predictions, predictions_ = model.feed(input)\n",
        "    # print(predictions)\n",
        "\n",
        "    # Converting to numpy\n",
        "    a = []\n",
        "    out = output.data.cpu().numpy()\n",
        "    for o in out:\n",
        "      a.append(int(o[0]))\n",
        "\n",
        "    # Taking out the max valued labels\n",
        "    pred, indices = torch.max(predictions, 1)\n",
        "\n",
        "    # Converting to long tensor type \n",
        "    a_list = torch.FloatTensor(a).cuda().long()\n",
        "\n",
        "    # Get the Loss\n",
        "    loss = cross_loss(predictions, a_list)\n",
        "\n",
        "    # Other metric performances using softmax returned output\n",
        "    accuracy = accuracy_score(a, indices.data.cpu())\n",
        "    recall = recall_score(a, indices.data.cpu(), average='macro', zero_division=0)\n",
        "    precision = precision_score(a, indices.data.cpu(), average='macro', zero_division=0)\n",
        "\n",
        "    if(train):\n",
        "      # Clear errors\n",
        "      optimizer.zero_grad()\n",
        "      # Compute the gradients for our optimizer\n",
        "      loss.backward()\n",
        "      # Using optimizer to update model's parameters based on gradients\n",
        "      optimizer.step()\n",
        "    \n",
        "    # Loss\n",
        "    avg_loss += loss.item()\n",
        "\n",
        "    # Other performance metrics\n",
        "    avg_accuracy += accuracy\n",
        "    avg_recall += recall\n",
        "    avg_precision += precision\n",
        "    count += 1\n",
        "  \n",
        "  return avg_loss/count, avg_accuracy/count, avg_recall/count, avg_precision/count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxK3luOgycHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_history = []\n",
        "f1_score_history = []\n",
        "loss_history = []\n",
        "accuracy_history = []\n",
        "\n",
        "# Training the model\n",
        "epochs = 50\n",
        "# optimizer = SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "# optimizer = Adamax(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Convert training set to torch variables using GPU as float.\n",
        "# The reshape is to remove PyTorch warning\n",
        "inputs = torch.from_numpy(x_train_np).cuda().float()\n",
        "outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()\n",
        "\n",
        "# Create DataLoader instance to work with batches\n",
        "tensor = TensorDataset(inputs, outputs)\n",
        "loader = DataLoader(tensor, batchsize, shuffle=True, drop_last=True)\n",
        "\n",
        "# Start training loop\n",
        "for epoch in range(epochs):\n",
        "  # Cycle through batches and get average loss and other metrics\n",
        "  avg_loss, avg_accuracy, avg_recall, avg_precision = model_loss(model, loader, train=True, optimizer=optimizer)\n",
        "  \n",
        "  # Get f1 score\n",
        "  f1 = (2 * (avg_precision * avg_recall)/(avg_precision + avg_recall))\n",
        "\n",
        "  # Saving the history for ploting on graph\n",
        "  epoch_history.append(epoch + 1)\n",
        "  f1_score_history.append(f1)\n",
        "  loss_history.append(avg_loss)\n",
        "  accuracy_history.append(avg_accuracy)\n",
        "\n",
        "  # Printing the loss and other metrics\n",
        "  print(\"Epoch \" + str(epoch + 1) + \n",
        "        \":\\n\\tLoss = \" + str(avg_loss) +\n",
        "        \"\\n\\tAccuracy = \" + str(avg_accuracy) +\n",
        "        \"\\n\\tRecall = \" + str(avg_recall) +\n",
        "        \"\\n\\tPrecision = \" + str(avg_precision) +\n",
        "        \"\\n\\tF1 Score = \" + str(f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrry5fT7kMtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving model\n",
        "torch.save(model, '1101352_1dconv_reg.pt') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvUFQ1m3kSOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading model\n",
        "model = torch.load('1101352_1dconv_reg.pt')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAaCWLgm_ClT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting Accuracy and F1 score together\n",
        "plt.plot(epoch_history, accuracy_history, label='Accuracy')\n",
        "plt.plot(epoch_history, f1_score_history, label='F1 score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Metrics')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Graph for Loss w.r.t Epoch\n",
        "plt.plot(epoch_history, loss_history, label='Loss', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tsxK3pjmGML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-processing for the Testing set\n",
        "X, Y = [], []\n",
        "\n",
        "# For every single label save the Sentiment label\n",
        "for y in range(len(test_ds)):\n",
        "  label = test_ds['Sentiment'][y]\n",
        "  temp_phrase = []\n",
        "\n",
        "  # Splitting the pharase to words and setting the text to lowercase\n",
        "  space = str(test_ds['Phrase'][y]).lower().split(' ')\n",
        "  # print(space)\n",
        "\n",
        "  # For each word in the Phrase\n",
        "  for word in space:\n",
        "    new_word = word\n",
        "\n",
        "    # Giving conditional procedures according to stopwords and punctuations\n",
        "    if(word in stopwords_en):\n",
        "      continue\n",
        "    if(word in punctuations):\n",
        "      continue\n",
        "    \n",
        "    # Condition specifying if stemming is used or not\n",
        "    if(use_stemming):\n",
        "      # Using either Porter or Lancaster stemming\n",
        "      new_word = porter_stemmer.stem(new_word)\n",
        "      # new_word = lancaster_stemmer.stem(new_word)\n",
        "    \n",
        "    # Condition specifying if lemmatization is used or not\n",
        "    if(use_lemmatization):\n",
        "      new_word = wordnet_lemmatizer.lemmatize(new_word)\n",
        "\n",
        "    # Adding normalized word to a temporary variable\n",
        "    temp_phrase.append(new_word)\n",
        "\n",
        "  # if(not temp_phrase == []):\n",
        "  X.append(temp_phrase)\n",
        "  Y.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_ub9ioknD5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UI1_e7XnDKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Joining the phrase\n",
        "for i in range(len(X)):\n",
        "  X[i] = ' '.join(X[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCwqARV7nMni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5b4DQaqnN1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM-7RkNVnPUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merging two lists into a tuple using zip() method and converting the lists of\n",
        "# tuples to Pandas DataFrame and setting the column name\n",
        "ds_test = pd.DataFrame(list(zip(X, Y)), columns=['Phrase', 'Sentiment'])\n",
        "\n",
        "# Replacing the blank cells with NaN in the DataFrame\n",
        "ds_test['Phrase'].replace('', np.nan, inplace=True)\n",
        "\n",
        "# Dropping all the NaN values from the DataFrame\n",
        "ds_test.dropna(subset=['Phrase'], inplace=True)\n",
        "\n",
        "# ds_test.head()\n",
        "len(ds_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UponSGNgns9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(ds_test.Sentiment.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IPQUnBQvXH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f0U-ivsoA3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorizing the phrases\n",
        "x_test = vectorizer.transform(ds_test[\"Phrase\"])\n",
        "y_test = ds_test[\"Sentiment\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgnGrJw_obY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting to numpy arrays\n",
        "x_test_np = x_test.toarray()\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "print(x_test_np.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpyCtNwLqasd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the model\n",
        "inputs = torch.from_numpy(x_test_np).cuda().float()\n",
        "outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()\n",
        "\n",
        "# Create DataLoader instance to work with batches\n",
        "tensor = TensorDataset(inputs, outputs)\n",
        "loader = DataLoader(tensor, batchsize, shuffle=True, drop_last=True)\n",
        "\n",
        "# Cycle through batches and get average loss and other metrics\n",
        "avg_loss, avg_accuracy, avg_recall, avg_precision = model_loss(model, loader)\n",
        "\n",
        "# Get f1 score\n",
        "f1 = (2 * (avg_recall * avg_precision)/(avg_recall + avg_precision))\n",
        "\n",
        "# Printing the loss and other metrics\n",
        "print(\"Testing Loss = \" + str(avg_loss) +\n",
        "    \"\\nTesting Accuracy = \" + str(avg_accuracy) + \n",
        "    \"\\nTesting Recall = \" + str(avg_recall) + \n",
        "    \"\\nTesting Precision = \" + str(avg_precision) + \n",
        "    \"\\nTesting F1 Score = \" + str(f1))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}